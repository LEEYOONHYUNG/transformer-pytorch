{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU util\n",
    "???/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "15000\n",
      "\n",
      "Elapsed time: 0:02:21.369614\n",
      "\n",
      "Data_length: 159022\n",
      "VOCA_SIZE: 15000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from data_prepare import TextPairs\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from torch.utils.data import DataLoader\n",
    "from RNN import RNN_encoder, RNN_decoder\n",
    "from Transformer import Transformer_encoder, Transformer_decoder\n",
    "from datetime import datetime\n",
    "from math import sin, cos\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 400 # max 400 on cuda:2\n",
    "VOCA_SIZE = 15000 # smaller than len(text_pairs.voca['en']), len(text_pairs.voca['de']) => 4000 / 15000 for toy / real\n",
    "NUM_LAYERS = 4\n",
    "NUM_HEADS = 8\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "# toy: 10,000 sentences / real: 176,692 sentences\n",
    "# 4473 ,6706 / 15151, 32829\n",
    "start = datetime.now()\n",
    "train_pairs = TextPairs(VOCA_SIZE, train=True, toy=False)\n",
    "val_pairs = TextPairs(VOCA_SIZE, train=False, toy=False)\n",
    "\n",
    "print( len(train_pairs.voca['en']) )\n",
    "print( len(train_pairs.voca['de']) )\n",
    "\n",
    "trainLoader = DataLoader(train_pairs, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "valLoader = DataLoader(val_pairs, num_workers=4)\n",
    "\n",
    "MAX_LEN = train_pairs.max_len\n",
    "SAMPLE = [15, 5015, 10015]\n",
    "print(f'\\nElapsed time: {datetime.now() - start}')\n",
    "print(f'\\nData_length: {len(train_pairs)}')\n",
    "print(f'VOCA_SIZE: {VOCA_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/Glove/glove.6B.200d.pkl', 'rb') as f:\n",
    "    glove = pkl.load(f)\n",
    "\n",
    "embedding_matrix = torch.zeros(VOCA_SIZE, EMBEDDING_DIM)\n",
    "\n",
    "for w in train_pairs.voca['en']:\n",
    "    if glove.get(w) is None:\n",
    "        embedding_matrix[ train_pairs.word2id['en'][w] ] = torch.zeros(EMBEDDING_DIM)\n",
    "    else:\n",
    "        embedding_matrix[ train_pairs.word2id['en'][w] ] = torch.from_numpy(glove.get(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PE = torch.zeros(1, MAX_LEN, EMBEDDING_DIM)\n",
    "for pos in range(MAX_LEN):\n",
    "    for i in range(EMBEDDING_DIM//2):\n",
    "        PE[0, pos, 2*i] = sin(pos / 10000**(2*i/EMBEDDING_DIM))\n",
    "        PE[0, pos, 2*i+1] = cos(pos / 10000**(2*i/EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 11106200\n"
     ]
    }
   ],
   "source": [
    "class TF_MODEL(nn.Module):\n",
    "    def __init__(self, NUM_LAYERS, NUM_HEADS, VOCA_SIZE, EMBEDDING_DIM, embedding_matrix, MAX_LEN):\n",
    "        super(TF_MODEL, self).__init__()\n",
    "        self.encoder = Transformer_encoder(NUM_LAYERS, NUM_HEADS, VOCA_SIZE, EMBEDDING_DIM, embedding_matrix)\n",
    "        self.decoder = Transformer_decoder(NUM_LAYERS, NUM_HEADS, VOCA_SIZE, EMBEDDING_DIM, MAX_LEN)\n",
    "        \n",
    "    def forward(self, encoder_inputs, PE, decoder_inputs, train):\n",
    "        context = self.encoder(encoder_inputs, PE)\n",
    "        preds = self.decoder(decoder_inputs, context, PE, train) # BATCH_SIZE, MAX_LEN, hidden_dim\n",
    "        return preds\n",
    "    \n",
    "\n",
    "model = TF_MODEL(NUM_LAYERS, NUM_HEADS, VOCA_SIZE, EMBEDDING_DIM, embedding_matrix, MAX_LEN)\n",
    "gpu = torch.device('cuda:2')\n",
    "cpu = torch.device('cpu')\n",
    "model = model.to(gpu)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-11 15:41:36.630144\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 1\t 2019-04-11 15:41:36.630764\n",
      "Pred:\t ['ich', 'bin', 'mit', 'mir', 'und', 'mich', ',', 'dass', 'sie', 'zu', 'hause']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['ein', '<unk>', 'kann', 'nicht', 'so']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'und', 'maria', 'und', 'maria', 'nicht', ',', 'was', 'er', 'hat']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t8.731\n",
      "Val loss:\t8.981\n",
      "BLEU score:\t 0.04031160874025352\n",
      "Pred:\t ['ich', 'bin', '<unk>', 'und', 'ich', 'etwas', 'essen']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['ein', 'mensch', 'kann', 'nicht', 'mehr', ',', 'als', 'ohne', '<unk>']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'und', 'maria', 'haben', 'keine', 'ahnung', ',', 'was', 'ich', 'mit', 'dem', 'mit', 'dem', 'johannes', 'mit', 'dem', 'johannes', 'zu', 'reden', 'hatte']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t8.252\n",
      "Val loss:\t8.931\n",
      "BLEU score:\t 0.08254206253813\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 2\t 2019-04-11 17:23:14.501229\n",
      "Pred:\t ['ich', 'werde', 'mich', '<unk>', ',', 'und', 'zu', 'essen']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['jemand', 'kann', 'kein', 'mensch']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'und', 'maria', 'hatten', 'keine', 'ahnung', ',', 'was', 'johannes', 'er', 'gerade', 'gesprochen']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t7.695\n",
      "Val loss:\t8.924\n",
      "BLEU score:\t 0.12543266402256914\n",
      "Pred:\t ['ich', 'werde', 'mich', 'beeilen', ',', 'und', 'gib', 'mir', 'etwas', 'zu', 'essen']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['ein', 'mensch', 'kann', 'nicht', 'jeder', ',', 'dass', 'es', 'v√∂llig', 'ist']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'und', 'maria', 'haben', 'beide', 'keine', 'ahnung', ',', 'was', 'johannes', 'hat']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t7.411\n",
      "Val loss:\t8.895\n",
      "BLEU score:\t 0.1375391468900233\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 3\t 2019-04-11 19:01:01.670445\n",
      "Pred:\t ['ich', 'werde', 'mich', '<unk>', 'und', 'geben', 'sie', 'etwas', 'zu', 'essen']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['ein', 'mensch', 'kann', 'nicht', 'jeder', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen', 'anderen']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'was', 'er', 'sprach', 'hatte']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t6.941\n",
      "Val loss:\t8.897\n",
      "BLEU score:\t 0.15657636363385913\n",
      "Pred:\t ['ich', 'habe', 'es', 'mich', '<unk>', ',', 'und', 'ich', 'brauche', 'etwas', 'zu', 'essen']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['ein', 'mensch', 'kann', 'nicht', 'jeder', 'andere']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'was', 'mit', 'dem', 'johannes', 'gesprochen', 'hat']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t6.679\n",
      "Val loss:\t8.872\n",
      "BLEU score:\t 0.16380943069840087\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 4\t 2019-04-11 20:47:01.028747\n",
      "Pred:\t ['ich', 'habe', 'angst', ',', 'um', 'etwas', 'zu', 'essen']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['eine', 'person', 'kann', 'das', 'einzige', 'nicht', 'verstehen']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'was', 'mit', 'dem', 'johannes']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t6.228\n",
      "Val loss:\t8.912\n",
      "BLEU score:\t 0.17662398554976133\n",
      "Pred:\t ['ich', 'f√ºrchte', ',', 'dass', 'ich', 'mich', 'etwas', 'zu', 'essen', 'geben']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['eine', 'person', 'kann', 'nicht', 'jeder', 'andere']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'und', 'maria', 'hatten', 'beide', ',', 'was', 'johannes', 'war']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t5.973\n",
      "Val loss:\t8.917\n",
      "BLEU score:\t 0.18246824678131077\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 5\t 2019-04-11 22:34:25.197656\n",
      "Pred:\t ['ich', 'kann', 'mich', 'beeilen', 'bringen', ',', 'etwas', 'zu', 'essen', 'geben']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['eine', 'person', 'kann', 'nicht', 'anders', 'verstehen']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'was', 'johannes', 'rede']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t5.528\n",
      "Val loss:\t8.883\n",
      "BLEU score:\t 0.1791344471452451\n",
      "Pred:\t ['ich', 'habe', 'mich', 'beeilen', 'und', 'geben', 'sie', 'mir', 'etwas', 'zu', 'essen']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['ein', 'mensch', 'kann', 'nicht', 'anders', 'verstehen']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'und', 'maria', 'hatten', 'keine', 'ahnung', ',', 'was', 'er', 'mit', 'dem', 'gespr√§ch', 'dar√ºber', 'sprach']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t5.276\n",
      "Val loss:\t8.928\n",
      "BLEU score:\t 0.1833437883384876\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "PE = PE.to(gpu)\n",
    "\n",
    "print(datetime.now())\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "BLEUS = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = 0\n",
    "    print(\"\\n\\n\")\n",
    "    print(f'Epoch: {epoch+1}\\t', datetime.now())\n",
    "    \n",
    "    for i, data in enumerate(trainLoader):\n",
    "        en_text, de_text = data['en'], data['de']\n",
    "        \n",
    "        encoder_inputs, decoder_inputs, targets = en_text, de_text[:,:-1], de_text[:,1:]\n",
    "        encoder_inputs = encoder_inputs.to(gpu)\n",
    "        decoder_inputs = decoder_inputs.to(gpu)\n",
    "        targets = targets.to(gpu)\n",
    "        \n",
    "        preds = model(encoder_inputs, PE, decoder_inputs, train=True) # BATCH_SIZE, MAX_LEN, hidden_dim\n",
    "        loss = criterion( preds.view(-1, VOCA_SIZE), targets.contiguous().view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += float(loss)/150\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 150==0: # len(trainLoader) = 398\n",
    "            train_losses.append(train_loss)\n",
    "            references, hypotheses = [], []\n",
    "            val_loss = 0\n",
    "            model = model.to(cpu)\n",
    "            PE = PE.to(cpu)\n",
    "            with torch.no_grad():\n",
    "                for j, data in enumerate(valLoader):\n",
    "                    en_text, de_text = data['en'], data['de']\n",
    "\n",
    "                    sos = torch.cat( [torch.tensor([[2]]), torch.zeros(1, MAX_LEN-1, dtype=torch.long)], dim=-1 )\n",
    "                    preds = model(en_text, PE, sos, train=False) # BATCH_SIZE, MAX_LEN, hidden_dim\n",
    "\n",
    "                    preds_loss = preds.new_zeros(MAX_LEN, VOCA_SIZE)\n",
    "                    preds_loss[:len(preds[0])] = preds[0]\n",
    "                    targets = de_text[:,1:]\n",
    "                    loss = criterion( preds_loss, targets.contiguous().view(-1))\n",
    "                    val_loss += float(loss)/len(valLoader)\n",
    "\n",
    "                    tokens = torch.argmax(preds[0], dim=-1)\n",
    "                    text = [ val_pairs.voca['de'][t] for t in tokens if t not in [0,2,3]]\n",
    "\n",
    "                    reference = val_pairs.hyp2ref[ ' '.join([val_pairs.voca['en'][t] for t in en_text[0] if t not in [0,2,3]]) ]\n",
    "                    hypothesis = text\n",
    "\n",
    "                    references.append(reference)\n",
    "                    hypotheses.append(hypothesis)\n",
    "\n",
    "                    if j in SAMPLE:\n",
    "                        print('Pred:\\t', hypothesis)\n",
    "                        print('Target:\\t', reference[0])\n",
    "                \n",
    "            val_losses.append(val_loss)\n",
    "            BLEUS.append(corpus_bleu(references, hypotheses))\n",
    "            print(f'Train loss:\\t{train_losses[-1]:.3f}')\n",
    "            print(f'Val loss:\\t{val_losses[-1]:.3f}')\n",
    "            print('BLEU score:\\t', BLEUS[-1])\n",
    "            train_loss=0\n",
    "            model = model.to(gpu)\n",
    "            PE = PE.to(gpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
