{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU util\n",
    "???/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "15000\n",
      "\n",
      "Elapsed time: 0:02:20.049706\n",
      "\n",
      "Data_length: 159022\n",
      "VOCA_SIZE: 15000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from data_prepare import TextPairs\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from torch.utils.data import DataLoader\n",
    "from RNN import RNN_encoder, RNN_decoder\n",
    "from Transformer import Transformer_encoder, Transformer_decoder\n",
    "from datetime import datetime\n",
    "from math import sin, cos\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 400\n",
    "VOCA_SIZE = 15000 # smaller than len(text_pairs.voca['en']), len(text_pairs.voca['de']) => 4000 / 15000 for toy / real\n",
    "NUM_LAYERS = 2\n",
    "HIDDEN_DIM = 256\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "# toy: 10,000 sentences / real: 176,692 sentences\n",
    "# 4473 ,6706 / 15151, 32829\n",
    "start = datetime.now()\n",
    "train_pairs = TextPairs(VOCA_SIZE, train=True, toy=False)\n",
    "val_pairs = TextPairs(VOCA_SIZE, train=False, toy=False)\n",
    "\n",
    "print( len(train_pairs.voca['en']) )\n",
    "print( len(train_pairs.voca['de']) )\n",
    "\n",
    "trainLoader = DataLoader(train_pairs, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "valLoader = DataLoader(val_pairs, num_workers=4)\n",
    "\n",
    "MAX_LEN = train_pairs.max_len\n",
    "SAMPLE = [15, 5015, 10015]\n",
    "print(f'\\nElapsed time: {datetime.now() - start}')\n",
    "print(f'\\nData_length: {len(train_pairs)}')\n",
    "print(f'VOCA_SIZE: {VOCA_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/Glove/glove.6B.200d.pkl', 'rb') as f:\n",
    "    glove = pkl.load(f)\n",
    "\n",
    "embedding_matrix = torch.zeros(VOCA_SIZE, EMBEDDING_DIM)\n",
    "\n",
    "for w in train_pairs.voca['en']:\n",
    "    if glove.get(w) is None:\n",
    "        embedding_matrix[ train_pairs.word2id['en'][w] ] = torch.zeros(EMBEDDING_DIM)\n",
    "    else:\n",
    "        embedding_matrix[ train_pairs.word2id['en'][w] ] = torch.from_numpy(glove.get(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 12881889\n"
     ]
    }
   ],
   "source": [
    "class RNN_MODEL(nn.Module):\n",
    "    def __init__(self, NUM_LAYERS, HIDDEN_DIM, VOCA_SIZE, EMBEDDING_DIM, embedding_matrix, MAX_LEN):\n",
    "        super(RNN_MODEL, self).__init__()\n",
    "        self.encoder = RNN_encoder(NUM_LAYERS, HIDDEN_DIM, VOCA_SIZE, EMBEDDING_DIM, embedding_matrix)\n",
    "        self.decoder = RNN_decoder(NUM_LAYERS, HIDDEN_DIM, VOCA_SIZE, EMBEDDING_DIM, MAX_LEN)\n",
    "        \n",
    "    def forward(self, encoder_inputs, decoder_inputs, train):\n",
    "        context = self.encoder(encoder_inputs)\n",
    "        preds = self.decoder(decoder_inputs, context, train) # BATCH_SIZE, MAX_LEN, hidden_dim\n",
    "        return preds\n",
    "\n",
    "\n",
    "model = RNN_MODEL(NUM_LAYERS, HIDDEN_DIM, VOCA_SIZE, EMBEDDING_DIM, embedding_matrix, MAX_LEN)\n",
    "gpu = torch.device('cuda:2')\n",
    "cpu = torch.device('cpu')\n",
    "model = model.to(gpu)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-12 09:50:37.630591\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 1\t 2019-04-12 09:50:37.631212\n",
      "Pred:\t ['ich', 'ist', ',', 'ich', 'ist', ',', 'ich', 'ist', ',', 'dass', 'ich', 'ist', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'tom']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['ich', 'ist', ',', 'ich', 'ist', ',', 'ich', 'ist', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'tom']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['ich', 'ist', ',', 'ich', 'ist', ',', 'ich', 'ist', ',', 'ich', 'ist', ',', 'dass', 'ich', 'ist', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'ich', ',', 'dass', 'tom']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t8.943\n",
      "Val loss:\t8.045\n",
      "BLEU score:\t 0.0023199470306732033\n",
      "Pred:\t ['ich', 'habe', 'das', ',', 'dass', 'du', 'das', ',', 'dass', 'du', 'das']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['ich', 'habe', 'das', ',', 'dass', 'du', 'das', ',', 'dass', 'du', 'das']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['ich', 'habe', 'das', ',', 'dass', 'du', 'das', ',', 'dass', 'du', 'das', ',', 'dass', 'du', 'zu']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t8.597\n",
      "Val loss:\t8.925\n",
      "BLEU score:\t 0.005338065582718432\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 2\t 2019-04-12 10:39:05.175763\n",
      "Pred:\t ['ich', 'habe', 'nicht', ',', 'dass', 'er', 'nicht', 'zu', 'hause']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['tom', 'ist', ',', 'dass', 'er', 'das', 'zu', 'hause']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'hat', 'sich', ',', 'dass', 'er', 'zu', '<unk>', 'zu', 'hause']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t8.103\n",
      "Val loss:\t9.090\n",
      "BLEU score:\t 0.011432583590133601\n",
      "Pred:\t ['ich', 'habe', 'nicht', ',', 'dass', 'er', 'zu', 'tun']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['das', 'ist', 'nicht', ',', 'dass', 'er', 'es', 'zu', 'tun']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'hat', 'sich', ',', 'dass', 'er', 'das', '<unk>', 'zu', 'tun', ',', 'was', 'er', 'zu', 'tun']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t7.806\n",
      "Val loss:\t9.075\n",
      "BLEU score:\t 0.015677468706802183\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 3\t 2019-04-12 11:22:23.788180\n",
      "Pred:\t ['ich', 'habe', 'mich', ',', 'dass', 'ich', 'mit', 'dir', 'zu', 'hause', ',', 'um', 'zu', 'tun']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['das', 'ist', 'nicht', 'ein', '<unk>', '<unk>']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'hat', 'maria', ',', 'dass', 'er', 'das', 'geld', ',', 'was', 'er', 'zu', 'tun']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t7.324\n",
      "Val loss:\t9.045\n",
      "BLEU score:\t 0.024250201170850836\n",
      "Pred:\t ['ich', 'habe', 'viel', 'für', 'etwas', 'zu', 'helfen']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['das', 'ist', 'nicht', 'so', 'viel', 'wie', 'ein', '<unk>']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'und', 'maria', 'haben', ',', 'dass', 'er', 'nicht', 'zu', 'spät', ',', 'maria', 'zu', 'sein']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t7.039\n",
      "Val loss:\t9.041\n",
      "BLEU score:\t 0.02994387583597524\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 4\t 2019-04-12 12:07:04.869277\n",
      "Pred:\t ['ich', 'liebe', 'mich', ',', 'dass', 'sie', 'sich', '<unk>', 'zu', 'helfen']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['das', 'ist', 'nicht', 'ein', '<unk>', '<unk>']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'und', 'maria', 'haben', 'sich', 'nicht', ',', 'dass', 'maria', 'zu', 'tun']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t6.562\n",
      "Val loss:\t9.032\n",
      "BLEU score:\t 0.038455017253484776\n",
      "Pred:\t ['ich', 'freue', 'mich', ',', 'dass', 'sie', 'sich', '<unk>', 'zu', 'essen']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['ein', '<unk>', 'kann', 'man', 'nicht', 'ein', 'guter', '<unk>']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'und', 'maria', 'haben', 'sich', 'nicht', 'mehr', ',', 'was', 'er', 'sich', 'nicht', '<unk>']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t6.284\n",
      "Val loss:\t8.993\n",
      "BLEU score:\t 0.04538529470282372\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 5\t 2019-04-12 12:54:10.494384\n",
      "Pred:\t ['ich', 'hoffe', ',', 'dass', 'sie', 'mir', 'etwas', 'zu', 'trinken', ',', 'um', 'zu', 'essen']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['ein', 'problem', 'kann', 'sich', 'nicht', 'eine', 'frage', 'zu', 'tun']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'und', 'maria', 'haben', 'sich', 'nie', 'über', 'maria', 'gesagt']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t5.817\n",
      "Val loss:\t8.992\n",
      "BLEU score:\t 0.055105500076800054\n",
      "Pred:\t ['ich', 'habe', 'mich', 'etwas', 'essen', ',', 'wenn', 'sie', 'sich', '<unk>']\n",
      "Target:\t ['ich', 'bin', 'am', 'verhungern', 'los', ',', 'gib', 'mir', 'schnell', 'etwas', 'zu', 'essen']\n",
      "Pred:\t ['ein', '<unk>', 'kann', 'man', 'nicht', 'ein', '<unk>']\n",
      "Target:\t ['eine', 'person', 'kann', 'eine', 'andere', 'person', 'nicht', 'vollkommen', 'verstehen']\n",
      "Pred:\t ['tom', 'und', 'maria', 'haben', 'uns', 'gesagt', ',', 'was', 'er', 'noch', 'nicht', 'wollte']\n",
      "Target:\t ['tom', 'und', 'maria', 'hatten', 'beide', 'keine', 'ahnung', ',', 'wovon', 'johannes', 'redete']\n",
      "Train loss:\t5.548\n",
      "Val loss:\t8.989\n",
      "BLEU score:\t 0.058261395561014645\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "print(datetime.now())\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "BLEUS = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = 0\n",
    "    print(\"\\n\\n\")\n",
    "    print(f'Epoch: {epoch+1}\\t', datetime.now())\n",
    "    \n",
    "    for i, data in enumerate(trainLoader):\n",
    "        en_text, de_text = data['en'], data['de']\n",
    "\n",
    "        encoder_inputs, decoder_inputs, targets = en_text, de_text[:,:-1], de_text[:,1:]\n",
    "        encoder_inputs = encoder_inputs.to(gpu)\n",
    "        decoder_inputs = decoder_inputs.to(gpu)\n",
    "        targets = targets.to(gpu)\n",
    "        \n",
    "        preds = model(encoder_inputs, decoder_inputs, train=True)\n",
    "        loss = criterion( preds.view(-1, VOCA_SIZE), targets.contiguous().view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += float(loss)/150\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 150==0: # len(trainLoader) = 398\n",
    "            train_losses.append(train_loss)\n",
    "            references, hypotheses = [], []\n",
    "            val_loss = 0\n",
    "            model = model.to(cpu)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for j, data in enumerate(valLoader):\n",
    "                    en_text, de_text = data['en'], data['de']\n",
    "\n",
    "                    sos = torch.tensor([[2]])\n",
    "                    preds = model( en_text, sos, train=False) # BATCH_SIZE, MAX_LEN, hidden_dim\n",
    "\n",
    "                    preds_loss = preds.new_zeros(MAX_LEN, VOCA_SIZE)\n",
    "                    preds_loss[:len(preds[0])] = preds[0]\n",
    "                    targets = de_text[:,1:]\n",
    "                    loss = criterion( preds_loss, targets.contiguous().view(-1))\n",
    "                    val_loss += float(loss)/len(valLoader)\n",
    "                    \n",
    "\n",
    "                    tokens = torch.argmax(preds[0], dim=-1)\n",
    "                    text = [ val_pairs.voca['de'][t] for t in tokens if t not in [0,2,3]]\n",
    "\n",
    "                    reference = val_pairs.hyp2ref[ ' '.join([val_pairs.voca['en'][t] for t in en_text[0] if t not in [0,2,3]]) ]\n",
    "                    hypothesis = text\n",
    "\n",
    "                    references.append(reference)\n",
    "                    hypotheses.append(hypothesis)\n",
    "\n",
    "                    if j in SAMPLE:\n",
    "                        print('Pred:\\t', hypothesis)\n",
    "                        print('Target:\\t', reference[0])\n",
    "\n",
    "            val_losses.append(val_loss)\n",
    "            BLEUS.append(corpus_bleu(references, hypotheses))\n",
    "            print(f'Train loss:\\t{train_losses[-1]:.3f}')\n",
    "            print(f'Val loss:\\t{val_losses[-1]:.3f}')\n",
    "            print('BLEU score:\\t', BLEUS[-1])\n",
    "            train_loss=0\n",
    "            model = model.to(gpu)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
